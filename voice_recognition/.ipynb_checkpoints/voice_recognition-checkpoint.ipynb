{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset we are working with is VoxCeleb1 dataset, an audio dataset of utterances from 1,251 different celebrities.\n",
    "It is quite large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-57bc48914925>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mroot_directory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/wav'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mbytes_to_gb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mst_size\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mroot_directory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'**/*'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Size of VoxCeleb1 dataset:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbytes_to_gb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'GB'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-57bc48914925>\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mroot_directory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/wav'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mbytes_to_gb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mst_size\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mroot_directory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'**/*'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Size of VoxCeleb1 dataset:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbytes_to_gb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'GB'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\AudioWorkshop\\lib\\pathlib.py\u001b[0m in \u001b[0;36mis_file\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1385\u001b[0m         \"\"\"\n\u001b[0;32m   1386\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1387\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mS_ISREG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mst_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1388\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1389\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_ignore_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\AudioWorkshop\\lib\\pathlib.py\u001b[0m in \u001b[0;36mstat\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1181\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[0mdoes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m         \"\"\"\n\u001b[1;32m-> 1183\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mowner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "root_directory = Path('./data/wav')\n",
    "bytes_to_gb = 10**(-9)\n",
    "size = sum(f.stat().st_size for f in root_directory.glob('**/*') if f.is_file())\n",
    "print('Size of VoxCeleb1 dataset:', str(size * bytes_to_gb), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We iterate over the VoxCeleb1 dataset, and for each audio file we will extract several features.\n",
    "For some visualizations, consider the following sample audio file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SAMPLE_AUDIO_PATH = './data/wav/id10001/1zcIwhmdeo4/00002.wav'\n",
    "\n",
    "import IPython\n",
    "import librosa\n",
    "import librosa.display as display\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sample_audio_file, sr = librosa.load(path=SAMPLE_AUDIO_PATH)\n",
    "Audio(SAMPLE_AUDIO_PATH, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Before we dive in, we will be using quite a few external libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.utils import shuffle\n",
    "import librosa\n",
    "import librosa.display as disp1\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import IPython.display as disp\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import argparse\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Now, let's extract some features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Tonnetz: lattice diagram representing tonal space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tonnetz = librosa.feature.tonnetz(sample_audio_file, sr)\n",
    "display.specshow(data=tonnetz, y_axis='tonnetz')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "Spectogram: visual representation of the frequencies of the signal over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "M = librosa.feature.melspectrogram(y=sample_audio_file, sr=sr)\n",
    "M_db = librosa.power_to_db(M, ref=np.max)\n",
    "img = librosa.display.specshow(M_db, y_axis='mel', x_axis='time', ax=ax)\n",
    "ax.set(title='Spectogram')\n",
    "fig.colorbar(img, ax=ax, format=\"%+2.f dB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% md\n"
    }
   },
   "source": [
    "Pitches: how high or low the voices is, closely related to frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pitches, magnitudes = librosa.piptrack(y=sample_audio_file, sr=sr)\n",
    "plt.plot(np.tile(np.arange(pitches.shape[1]), [100, 1]).T, pitches[:100, :].T, '.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% md\n"
    }
   },
   "source": [
    "Tempogram: variation of tempo (pace) of the audio signal over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tempogram = librosa.feature.tempogram(sample_audio_file, sr)\n",
    "display.specshow(tempogram, sr=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% md\n"
    }
   },
   "source": [
    "MFCC: achieved by taking Fourier transform of the signal, applying log, then taking discrete cosine transform on the powers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mfcc = librosa.feature.mfcc(sample_audio_file, sr)\n",
    "display.specshow(mfcc, sr=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% md\n"
    }
   },
   "source": [
    "Chroma STFT: variation of time frequency distribution, similar to spectogram for 12 pitch classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "chroma_stft = librosa.feature.chroma_stft(sample_audio_file, sr)\n",
    "display.specshow(chroma_stft, sr=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% md\n"
    }
   },
   "source": [
    "Spectral rolloff: frequency below which 85% of the energy lies over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Normalising for visualisation\n",
    "import sklearn\n",
    "spectral_centroids = librosa.feature.spectral_centroid(x, sr=sr)[0]\n",
    "frames = range(len(spectral_centroids))\n",
    "\n",
    "def normalize(x, axis=0):\n",
    "    return sklearn.preprocessing.minmax_scale(x, axis=axis)\n",
    "\n",
    "spectral_rolloff = librosa.feature.spectral_rolloff(sample_audio_file, sr=sr)[0]\n",
    "librosa.display.waveplot(sample_audio_file, sr=sr, alpha=0.4)\n",
    "plt.plot(librosa.frames_to_time(frames), normalize(spectral_rolloff), color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## For every feature, we assign a weight: the weights can be played around with and achieved through trial and error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MAX_TONNETZ_WEIGHT = 450\n",
    "MAX_SPECTOGRAM_WEIGHT = 1000\n",
    "MAX_TEMPOGRAM_WEIGHT = 300\n",
    "MAX_CHROMA_WEIGHT = 400\n",
    "MAX_MFCC_WEIGHT = 500\n",
    "MAX_PITCH_WEIGHT = 500 \n",
    "MAX_SPEC_ROLLOF_WEIGHT = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, we need to load the VoxCeleb dataset.\n",
    "\n",
    "As we are dealing with a large dataset, we will process it with threads.\n",
    "\n",
    "We will also limit the number of files we process for each speaker, and limit the number of speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DatasetLoader:\n",
    "    def signal_handler(self, sig, frame):\n",
    "        global do_process\n",
    "        do_process = False\n",
    "        print('clicked ctrl+C')\n",
    "\n",
    "    def __init__(self, num_threads=5, max_files_to_process_per_speaker=10, min_audio_file_duration=8, max_numbers_of_speakers=5):\n",
    "        self.X = []\n",
    "        self.Y = []\n",
    "        self.threads = list()\n",
    "        self.file_paths_to_process = list()\n",
    "        self.num_threads = num_threads\n",
    "        self.lock = threading.Lock()\n",
    "        self.do_process = True\n",
    "        self.max_files_to_process_per_speaker = max_files_to_process_per_speaker\n",
    "        self.progress = None\n",
    "        self.min_audio_file_duration = min_audio_file_duration\n",
    "        self.max_numbers_of_speakers = max_numbers_of_speakers\n",
    "\n",
    "    def load_dataset(self, path):\n",
    "        for dir in os.listdir(path):\n",
    "            processed_count = 0\n",
    "            if processed_count > self.max_files_to_process_per_speaker:\n",
    "                continue\n",
    "            if int(dir[2:]) > 10000 + self.max_numbers_of_speakers:\n",
    "                return\n",
    "            if os.path.isdir(path + '/' + dir):\n",
    "                for inner_dir in os.listdir(path + '/' + dir):\n",
    "                    if processed_count > self.max_files_to_process_per_speaker:\n",
    "                        break\n",
    "                    for audio_file_to_process in os.listdir(path + '/' + dir + '/' + inner_dir):\n",
    "                        full_path = path + '/' + dir + '/' + inner_dir\n",
    "                        if librosa.get_duration(librosa.load(full_path + '/' + audio_file_to_process, sr=None)[0]) < self.min_audio_file_duration:\n",
    "                            continue\n",
    "                        self.file_paths_to_process.append((full_path, audio_file_to_process,))\n",
    "                        processed_count += 1\n",
    "                        if processed_count > self.max_files_to_process_per_speaker:\n",
    "                            break\n",
    "\n",
    "    def process_dataset(self):\n",
    "        split = np.array_split(self.file_paths_to_process, self.num_threads)\n",
    "        signal.signal(signal.SIGINT, self.signal_handler)\n",
    "        print('processing dataset with {0} threads and {1} total audio files'.format(self.num_threads, len(self.file_paths_to_process)))\n",
    "        self.progress = tqdm.tqdm(total=len(self.file_paths_to_process), desc='Extracting features from audio')\n",
    "\n",
    "        with self.progress:\n",
    "            for i in range(self.num_threads):\n",
    "                t = threading.Thread(target=self.process_audio_files_array, args=([split[i]]))\n",
    "                self.threads.append(t)\n",
    "                t.start()\n",
    "            for t in self.threads:\n",
    "                t.join()\n",
    "\n",
    "    def load_and_process_dataset(self, path):\n",
    "        self.load_dataset(path)\n",
    "        start = time.time()\n",
    "        self.process_dataset()\n",
    "        end = time.time()\n",
    "        print('finished processing dataset in {:.3f} ms'.format(end - start))\n",
    "        return self\n",
    "\n",
    "    def process_audio_files_array(self, arr):\n",
    "        for filepath, filename in arr:\n",
    "            self.process_audio_file(filepath, filename)\n",
    "\n",
    "    def process_audio_file(self, filepath, filename: str):\n",
    "        if not do_process:\n",
    "            return\n",
    "        label = filepath.split('/')[-2]\n",
    "        features = self.extract_features(filepath, filename)\n",
    "        self.lock.acquire()\n",
    "        self.X.append(features)\n",
    "        self.Y.append(label)\n",
    "        self.progress.update(1)\n",
    "        self.lock.release()\n",
    "\n",
    "    def extract_features(self, filepath, filename):\n",
    "        full_path = filepath + '/' + filename\n",
    "        if not os.path.isfile(full_path):\n",
    "            raise Exception('invalid path: ' + full_path)\n",
    "        samples, sample_rate = librosa.load(full_path, sr=None, duration=5)\n",
    "        tonnetz = librosa.feature.tonnetz(y=samples, sr=sample_rate)\n",
    "        chroma_stft = librosa.feature.chroma_stft(y=samples, sr=sample_rate)\n",
    "        tempogram = librosa.feature.tempogram(y=samples, sr=sample_rate)\n",
    "        spectogram = librosa.feature.melspectrogram(samples, sample_rate)\n",
    "        mfcc = librosa.feature.mfcc(y=samples, sr=sample_rate)\n",
    "        spec_rolloff = librosa.feature.spectral_rolloff(y=samples, sr=sample_rate)\n",
    "        pitches, magnitudes = librosa.piptrack(y=samples, sr=sample_rate)\n",
    "        return np.concatenate([tonnetz.flatten()[:MAX_TONNETZ_WEIGHT], spectogram.flatten()[:MAX_SPECTOGRAM_WEIGHT],\n",
    "                               tempogram.flatten()[:MAX_TEMPOGRAM_WEIGHT],\n",
    "                               chroma_stft.flatten()[:MAX_CHROMA_WEIGHT],\n",
    "                               mfcc.flatten()[:MAX_MFCC_WEIGHT], spec_rolloff.flatten()[:MAX_SPEC_ROLLOF_WEIGHT],\n",
    "                               magnitudes.flatten()[:MAX_PITCH_WEIGHT]])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After loading the dataset, we need a classifier - formally, a function from the domain of extracted audio samples features to the domain of labels.\n",
    "\n",
    "We try out the followings classifiers:\n",
    "\n",
    "- KNN: classify according to majority of nearest neighbors according to Euclidean distance\n",
    "- SVM: the classifier is a line that achieves the largest separating margin, found by solving a linear set of equations with constraints. Two types of kernel are tested out.\n",
    "- DT: a tree is grown such that every node is split according to some feature, and the leaves hold the decisions\n",
    "- NN: a deep neural network\n",
    "- Bayes: using Bayes theorem to solve the conditional probability for the output given the input \n",
    "- AdaBoost: iterative algorithm that trains an ensemble of weak learners, such that high weights are given to incorrect predictions\n",
    "\n",
    "The classifier is outputted from the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "CLASSIFIERS = ['knn', 'linear_svm', 'poly_svm', 'rbf', 'dt', 'nn', 'bayes', 'ada']\n",
    "\n",
    "def get_classifier(classifier):\n",
    "    if classifier not in CLASSIFIERS:\n",
    "        raise Exception('No classifier found!!')\n",
    "    print('classifier:', classifier)\n",
    "    if classifier == 'knn':\n",
    "        return KNeighborsClassifier(n_neighbors=1)\n",
    "    if classifier == 'linear_svm':\n",
    "        return SVC(kernel='linear')\n",
    "    if classifier == 'poly_svm':\n",
    "        return SVC(kernel='poly')\n",
    "    if classifier == 'rbf':\n",
    "        return SVC(kernel='rbf')\n",
    "    if classifier == 'dt':\n",
    "        return DecisionTreeClassifier()\n",
    "    if classifier == 'nn':\n",
    "        return MLPClassifier()\n",
    "    if classifier == 'bayes':\n",
    "        return GaussianNB()\n",
    "    if classifier == 'ada':\n",
    "        return AdaBoostClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After we have a classifier, we want to test its accuracy on the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test_accuracy(X, Y, classifier):\n",
    "    err = 0\n",
    "    prediction = classifier.predict(X)\n",
    "    for i in range(len(prediction)):\n",
    "        y, pred = Y[i], prediction[i]\n",
    "        print('model predicted:', pred, 'actual label:', y)\n",
    "        if pred != y:\n",
    "            err += 1\n",
    "    return 1 - (err / len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The entire flow happens in the main, where we check out every classifier and compare the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    train_path = 'data/wav'\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-c')\n",
    "    args = parser.parse_args()\n",
    "    classifier = get_classifier(classifier=args.c)\n",
    "    dataset_loader = DatasetLoader()\n",
    "    dataset_loader.load_and_process_dataset(train_path)\n",
    "    assert len(dataset_loader.X) == len(dataset_loader.Y)\n",
    "    # X, Y = shuffle(dataset_loader.X, dataset_loader.Y, random_state=42)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        dataset_loader.X, dataset_loader.Y, test_size=0.20, random_state=42)\n",
    "    for classifier in CLASSIFIERS:\n",
    "        c = get_classifier(classifier)\n",
    "        c.fit(X_train, y_train)\n",
    "        acc = test_accuracy(X_test, y_test, c)\n",
    "        print('done with accuracy: {:.3f}'.format(acc))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
